---
title: "EDA for Insurance Data"
output:
  pdf_document: default
  html_document: default
  word_document: default
date: '2022-06-15'
---

In this markdown, we will describe the steps of a classical exploratory analysis for Insurance data.
The data in use come from the chapter one of the book "Predictive Modelling Applications in Actuarial Science, Vol.2", Edited by E. Frees et al..The website is at the following address: https://instruction.bus.wisc.edu/jfrees/jfreesbooks/PredictiveModelingVol1/glm/v2-chapter-1.html


1. Data load
```{r}
# Define columnn class for dataset
colCls <- c("integer",         # row id
            "character",       # analysis year
            "numeric",         # exposure
            "character",       # new business / renewal business
            "numeric",         # driver age (continuous)
            "character",       # driver age (categorical)
            "character",       # driver gender
            "character",       # marital status
            "numeric",         # years licensed (continuous)
            "character",       # years licensed (categorical)
            "character",       # ncd level
            "character",       # region
            "character",       # body code
            "numeric",         # vehicle age (continuous)
            "character",       # vehicle age (categorical)
            "numeric",         # vehicle value
            "character",       # seats
            rep("numeric", 6), # ccm, hp, weight, length, width, height (all continuous)
            "character",       # fuel type
            rep("numeric", 3)  # prior claims, claim count, claim incurred (all continuous)
)
```

```{r}
# Define the data path and filename
data.path <- "C:\\Users\\William.Tiritilli\\Documents\\Project P\\Frees\\Tome 2 - Chapter 1\\"
data.fn <- "sim-modeling-dataset2.csv"

# Read in the data with the appropriate column classes
dta <- read.csv(paste(data.path, data.fn, sep = "/"),
                colClasses = colCls)
str(dta)
```

```{r}
set.seed(54321) # reproducibility
# Create a stratified data partition
train_id <- caret::createDataPartition(
  y = dta$clm.count/dta$exposure,
  p = 0.8,
  groups = 100
)[[1]]
```

```{r}
# Divide the data in training and test set
dta_trn <- dta[train_id,]
dta_tst <- dta[-train_id,]
```

```{r}
library(dplyr)
# Proportions of the number of claims in train data
dta_trn$clm.count %>% table %>% prop.table %>% round(5)
# Proportions of the number of claims in test data
dta_tst$clm.count %>% table %>% prop.table %>% round(5)
```
Proportions in train and test sets are well balanced.

We usually start our work by exploring individual variables to gain a better understanding on the info present 
in our dataset.

- clm.count: number of claims
- clm.incurred: the ultimate cost of those claims


EDA for Frequency
```{r}
# Create a summary table of frequency and severity
# by analysis period
yr.expo <- with(dta, tapply(exposure, year, sum)) #1. select the data, and 2. apply the sum of exposure by year
yr.clm.count <- with(dta, tapply(clm.count, year, sum)) # count the claims accross the year
yr.clm.incr <- with(dta, tapply(clm.incurred, year, sum))

yr.summary <- cbind(
  exposure = round(yr.expo,1),
  clm.count = yr.clm.count,
  clm.incurred = round(yr.clm.incr,0),
  frequency = round(yr.clm.count / yr.expo, 3),
  severity = round(yr.clm.incr / yr.clm.count, 1))

yr.summary <- rbind(yr.summary,
                    total = c(
                      round(sum(yr.expo),1),
                      sum(yr.clm.count),
                      round(sum(yr.clm.incr),0),
                      round(sum(yr.clm.count)/sum(yr.expo),3),
                      round(sum(yr.clm.incr)/sum(yr.clm.count),1)))
print(yr.summary)
```

We want to show the Evolution of the Empirical frequency and Exposure for all three years of the training data by driver age.
```{r}
library(ggplot2)
library(dplyr)

# Creation of the data frame
graph_data <- dta %>%  group_by(driver.age) %>%  summarise(Sum_Expo = sum(exposure),
                                               Number_of_Claims = sum(clm.count),
                                               Emp_freq = sum(clm.count)/sum(exposure)) 

# Bar plot overlapping with bar chart

# A few constants
freqColor <- "red"
expoColor <- rgb(0.2, 0.6, 0.9, 1)


# For the different scales,
# Set the following two values to values close to the limits of the data
# you can play around with these to adjust the positions of the graphs; 
# the axes will still be correct)
ylim.prim <- c(0, 1)      # for claim frequency
ylim.sec <- c(0, 500)     # for Exposure --> need to go way above the max to let 
                          # the data appearing in the chart
# For explanation: 
# https://stackoverflow.com/questions/32505298/explain-ggplot2-warning-removed-k-rows-containing-missing-values

# The following makes the necessary calculations based on these limits, 
# and makes the plot itself:
b <- diff(ylim.prim)/diff(ylim.sec)
a <- ylim.prim[1] - b*ylim.sec[1]

# Building the graph
graph_freq <- ggplot(graph_data, aes(x=driver.age, Emp_freq)) +
  
  geom_line( aes(y=Emp_freq), size=1, color=freqColor) +
  
    geom_bar( aes(y=a+Sum_Expo*b), stat="identity", size=.1, fill=expoColor, color="black", alpha=.4) +
  
  scale_y_continuous(
    
    # Features of the first axis
    name = "Empirical Frequency", limits = c(0, 1.5), 
  
    # Add a second axis and specify its features
    sec.axis = sec_axis(~ (. - a)/b, name = "Exposure")
  ) + 
  
  #theme_ipsum() +
  theme(
    axis.title.y = element_text(color = freqColor, size=13),
    axis.title.y.right = element_text(color = expoColor, size=13)
  ) +

  ggtitle("Empirical Claims Frequency by Driver Age")

# Print the whole graph
graph_freq 
```
The frequency decreases over years and become more volatile after 75 years old.

We want to see the pattern for each calendar year. A minor update of the previous code is required.
```{r}
# Creation of the data frame
graph_data2 <- dta %>%  group_by(driver.age, year) %>%  summarise(Sum_Expo = sum(exposure),
                                               Number_of_Claims = sum(clm.count),
                                               Emp_freq = sum(clm.count)/sum(exposure))
# Sort the dataframe by year
# https://dplyr.tidyverse.org/reference/arrange.html
graph_data2 <- arrange(graph_data2, year)                                   
head(graph_data2)


# A few constants
freqColor <- c("#D43F3A", "#EEA236", "#5CB85C", "#46B8DA", "#9632B8")
expoColor <- rgb(0.2, 0.6, 0.9, 1)


# For the different scales,
# Set the following two values to values close to the limits of the data
# you can play around with these to adjust the positions of the graphs; 
# the axes will still be correct)
ylim.prim <- c(0, 1)      # for claim frequency
ylim.sec <- c(0, 500)    # for Exposure --> need to go way above the max to let 
                          # the data appearing in the chart
# For explanation: 
# https://stackoverflow.com/questions/32505298/explain-ggplot2-warning-removed-k-rows-containing-missing-values

# The following makes the necessary calculations based on these limits, 
# and makes the plot itself:
b <- diff(ylim.prim)/diff(ylim.sec)
a <- ylim.prim[1] - b*ylim.sec[1]

# Building the graph
graph_freq <- ggplot(graph_data2, aes(x=driver.age, year, y = Emp_freq, color=year)) +
  
  geom_line( aes(y=Emp_freq), size=1) +
  
  scale_color_manual(values = freqColor) + 
  
    geom_bar( aes(y=a+Sum_Expo*b), stat="identity", size=.1, fill=expoColor, color="black", alpha=.4) +
  

  
  scale_y_continuous(
    
    # Features of the first axis
    name = "Empirical Frequency", limits = c(0, 1.5), 
  
    
    # Add a second axis and specify its features
    sec.axis = sec_axis(~ (. - a)/b, name = "Exposure")
  ) + 
  
  #theme_ipsum() +
  theme(
    axis.title.y = element_text(color = "green", size=13),
    axis.title.y.right = element_text(color = expoColor, size=13)
  ) +

  ggtitle("Empirical Claims Frequency by Driver Age")

# Print the whole graph
graph_freq 
```
We observe that 2013 and 2012 are very volatile for the younger age and the seniors. 
Moreover, the individual calendar year frequency are more volatile than all the 4 years combined.

Let's investigate some other variables, like the size of the engine (ccm).
It is a continuous variable, so we can split it by ranges using the function 'cut'.
The package dplyr will help to summarize the information.
```{r}
# Size of Engine
# Creation of a categorical

# Check the quantile
ccm_quantile <- quantile(dta$ccm)
print(ccm_quantile)

dta$ccm_range <- cut(dta$ccm, breaks = c(ccm_quantile[1],
                                                         ccm_quantile[2],
                                                         ccm_quantile[3],
                                                         ccm_quantile[4],
                                         ccm_quantile[5]),
                       labels = c("970-1398","1399-1560", 
                                  "1561-1896", "1897-3198"),include.lowest = TRUE)

# Use of the pipe to pivot the data
dta %>% group_by(ccm_range) %>% summarise(Sum_Expo = sum(exposure),
                                               Number_of_Claims = sum(clm.count),
                                               Emp_freq = sum(clm.count)/sum(exposure))
```

```{r}
# Example with another granularity
bk <- unique(quantile(dta$ccm, probs = seq(0, 1, by = 0.05)))
dta$ccm.d <- cut(dta$ccm, breaks = bk, include.lowest = TRUE)

dta %>% group_by(ccm.d) %>% summarise(Sum_Expo = sum(exposure),
                                               Number_of_Claims = sum(clm.count),
                                               Emp_freq = sum(clm.count)/sum(exposure))
```

Even if the practice of including the driver gender is not allowed in every country, it might be an interesting predictor to analyze the frequency of accidents.
```{r}
# Investigate the frequency of claims by the variables driver.gender and marital 
library(tidyr)

# https://www.youtube.com/watch?v=AkaiM-Mm_Ag
dta %>% group_by(driver.gender, year) %>% 
  summarise(emp_freq = round(sum(clm.count)/sum(exposure),3)*100) %>% 
  spread(driver.gender, emp_freq)

dta %>% group_by(marital.status, year) %>% 
  summarise(emp_freq = sum(clm.count)/sum(exposure)) %>% 
  spread(marital.status, emp_freq)
```
Totals needed here.


This line gives a quick view of the proportion between gender
```{r}
with (dta , table ( driver.gender, clm.count) )
```

Over the dataset, male drivers have a frequency equal to 15.8%, and females have had a frequency equal to 18.4%. This suggests that gender is a variable that could help segment our policyholders.

Is the difference significant?
We will randomly assign the label "married" to 22761 observations.
Then, we compute the frequency of each group and take the difference.

```{r}
# Creation of a train set
smp_size <- floor(0.7 * nrow(dta))
# set the seed to make your partition reproductible
set.seed(1234)
train_ind <- sample(seq_len(nrow(dta)), size = smp_size)
train<-dta[train_ind, ]
test<-dta[-train_ind, ]
```


```{r}
set.seed(1029384756)

# We want to run the experiement 10000 times
N <- 10000

tmp <- subset(train, marital.status %in% c("Married", "Widow"), select =c("marital.status",
"exposure", "clm.count") )

# Create a dataframe tagging each label with TRUE or FALSE
f <- tmp$marital.status == "Married"

# Create an empty dataframe of size N
d <- numeric(N)

for(i in 1:N) {
  
# fct sample takes a sample of the specified size from the elements of x
g <- sample(f, length(f))

#
married.fq <- sum(tmp$clm.count[g]) / sum(tmp$exposure[g])
widow.fq <- sum(tmp$clm.count[!g]) / sum(tmp$exposure[!g])


# Compute the difference in frequencies between married and widow
# and store in a data frame of size N
d[i] <- married.fq - widow.fq

}
```


Results
```{r}
quantile(d, c(0.025, 0.05, 0.1, 0.25, 0.5,
0.75, 0.9, 0.95, 0.975))
```
The confidence interval at 90% is (-0.041, 0.039). The actual difference we have observed of −0.115 is clearly outside this interval;
therefore, this difference is statistically significant. 

We can verify this with a graph:
```{r}
hist(d, breaks = c(20), main = paste("Simulated frequency difference between married and widowed drivers"))
```

The actual difference between these groups is well outside the bulk of the distribution.

Variable age is crucial for pricing. For a GLM regression, it is easier to bin the age variable into classes. In practice, an insurance product is designed in collaboration between  all the player of the company: Actuaries, marketing, sales...Each department plays its partition, with sometime divergence of interest. While Marketers and Sales want to sell to everybody at a competitive price, Actuaries would prefer alerting about the risks of under reserving and potential future losses. Sometimes, push back comes from an IT level because the pricing grid and different options would be too complex to implement in production.
As Golburg et al. says in the CAS Mongraph "Generalized linear model for insurance rating", "choosing between 
two final models is very often a business decision."

```{r}
dta$age.bins <- cut(dta$driver.age, c(0, 34, 64, 110))

dta %>% group_by(age.bins, driver.gender) %>% 
  summarise(emp_freq = sum(clm.count)/sum(exposure)) %>% 
  spread(age.bins, emp_freq)
```

Number of records by number of claims for the entire dataset. 
Statistics for new and renewal business.
```{r}
table(dta$clm.count)

dta %>% group_by(nb.rb) %>% 
  summarise(clm_inc = sum(clm.incurred),
            clm.cnt = sum(clm.count),
            severity = clm_inc/clm.cnt) %>% as.data.frame()
```


# Frequency Modeling

To apply a GLM, we need to make two choices: link function and response distribution.
For most insurance pricing, we would like to have a multiplicative rating plan so we will be using a logarithm link function.
Concerning the distribution, we are modeling a claim count, so the natural candidate are Poisson or Negative Binomial.

Claims distribution
```{r}
KULBg = "green"
# same graph with the weight of the expo
g1 <- ggplot(dta, aes(clm.count)) + theme_bw()+
  geom_bar( col = KULBg, fill = KULBg) +
  labs(y="Abs frequency")+
  ggtitle("Claims distribution")

print(g1)
```

Let's check if the assumption of Poisson having mean=variance are respected.

```{r}
f <- with(dta, clm.count / exposure) # frequency for each record
w <- with(dta, exposure) # weight for each record
mean.f <- sum(f * w) / sum(w) # mean frequency
second.f <- sum(f**2 * w) / sum(w) # second moment
var.f <- second.f - (mean.f)**2
print(var.f)
print(mean.f)
```
We can see that mean and variance are not equal. In this case the variance is large than the mean, and we have an overdispersed dataset.
